{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "\n",
    "import turicreate as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = tc.SFrame('amazon_baby.sframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Jupyter notebook above, we used the word counts for all words in the reviews to train the sentiment classifier model. Now, we are going to follow a similar path, but only use this subset of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = ['awesome', 'great', 'fantastic', 'amazing',\n",
    "                  'love', 'horrible', 'bad', 'terrible', 'awful',\n",
    "                  'wow', 'hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use .apply() to build a new feature with the counts for each of the selected_words:** In the notebook above, we created a column ‘word_count’ with the word counts for each review. Our first task is to create a new column in the products SFrame with the counts for each selected_word above, and, in the process, we will see how the method .apply() can be used to create new columns in our data (our features) and how to use a Python function, which is an extremely useful concept to grasp! \n",
    "\n",
    "Our first goal is to create a column products[‘awesome’] where each row contains the number of times the word ‘awesome’ showed up in the review for the corresponding product, and 0 if the review didn’t show up. One way to do this is to look at the each row ‘word_count’ column and follow this logic: \n",
    "\n",
    "+ If ‘awesome’ shows up in the word counts for a particular product (row of the products SFrame), then we know how often ‘awesome’ appeared in the review, \n",
    "+ if ‘awesome’ doesn’t appear in the word counts, then it didn’t appear in the review, and we should set the count for ‘awesome’ to 0 in this review.\n",
    "+ First, you will use a Python function to define the logic above. You will write a function called awesome_count which takes in the word counts and returns the number of times ‘awesome’ appears in the reviews.\n",
    "+ Next, you will use .apply() to iterate awesome_count for each row of products[‘word_count’] and create a new column called ‘awesome’ with the resulting counts.\n",
    "+ Repeat this process for the other 11 words in selected_words. (Here, we described a simple procedure to obtain the counts for each selected_word. There are other more efficient ways of doing this, and we encourage you to explore this further.)\n",
    "+ Using the .sum() method on each of the new columns you created, answer the following questions: Out of the selected_words, which one is most used in the dataset? Which one is least used? Save these results to answer the quiz at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = tc.text_analytics.count_words(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in selected_words:\n",
    "    data[word] = data['word_count'].apply(lambda counts: counts.get(word, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_values = {}\n",
    "for word in selected_words:\n",
    "    sum_values[word] = data[word].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used word:  great\n",
      "Least used word: wow\n"
     ]
    }
   ],
   "source": [
    "print(f'Most used word:  {max(sum_values, key=lambda k: sum_values[k])}')\n",
    "print(f'Least used word: {min(sum_values, key=lambda k: sum_values[k])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new sentiment analysis model using only the selected_words as features:** In the Jupyter Notebook above, we used word counts for all words as features for our sentiment classifier. Now, you are just going to use the selected_words:\n",
    "+ Use the same train/test split as in the Jupyter Notebook from lecture\n",
    "+ Train a logistic regression classifier (use turicreate.logistic_classifier.create) using just the selected_words. Hint: you can use this parameter in the .create() call to specify the features used to be exactly the new columns you just created\n",
    "+ You will now examine the weights the learned classifier assigned to each of the 11 words in selected_words and gain intuition as to what the ML algorithm did for your data using these features. In Turi Create, a learned model, such as the selected_words_model, has a field 'coefficients', which lets you look at the learned coefficients.\n",
    "+ Using this approach, sort the learned coefficients according to the ‘value’ column using .sort(). Out of the 11 words in selected_words, which one got the most positive weight? Which one got the most negative weight? Do these values make sense for you? Save these results to answer the quiz at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment based on review starts\n",
    "# Remove undefined sentiment\n",
    "data = data[data['rating'] != 3]\n",
    "# Positive sentiment 4- 5-star reviews\n",
    "data['sentiment'] = data['rating'] >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.random_split(0.8, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Logistic regression:</pre>"
      ],
      "text/plain": [
       "Logistic regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 133448</pre>"
      ],
      "text/plain": [
       "Number of examples          : 133448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of classes           : 2</pre>"
      ],
      "text/plain": [
       "Number of classes           : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of feature columns   : 11</pre>"
      ],
      "text/plain": [
       "Number of feature columns   : 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 11</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients      : 12</pre>"
      ],
      "text/plain": [
       "Number of coefficients      : 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training Accuracy | Validation Accuracy |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training Accuracy | Validation Accuracy |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 1.081362     | 0.847401          | 0.845874            |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 1.081362     | 0.847401          | 0.845874            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 2         | 3        | 1.131590     | 0.847514          | 0.846085            |</pre>"
      ],
      "text/plain": [
       "| 2         | 3        | 1.131590     | 0.847514          | 0.846085            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 3         | 4        | 1.176991     | 0.847626          | 0.846115            |</pre>"
      ],
      "text/plain": [
       "| 3         | 4        | 1.176991     | 0.847626          | 0.846115            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 4         | 5        | 1.229572     | 0.847708          | 0.846385            |</pre>"
      ],
      "text/plain": [
       "| 4         | 5        | 1.229572     | 0.847708          | 0.846385            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 5         | 6        | 1.274852     | 0.847708          | 0.846385            |</pre>"
      ],
      "text/plain": [
       "| 5         | 6        | 1.274852     | 0.847708          | 0.846385            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 6         | 7        | 1.320143     | 0.847708          | 0.846385            |</pre>"
      ],
      "text/plain": [
       "| 6         | 7        | 1.320143     | 0.847708          | 0.846385            |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+-------------------+---------------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+-------------------+---------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_words_model = tc.logistic_classifier.create(train_data,\n",
    "                                                     features=selected_words,\n",
    "                                                     target='sentiment',\n",
    "                                                     validation_set=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------+-----------------------+----------------------+\n",
      "|     name    | index | class |         value         |        stderr        |\n",
      "+-------------+-------+-------+-----------------------+----------------------+\n",
      "|     love    |  None |   1   |   1.3592688669225153  | 0.028068300152099435 |\n",
      "| (intercept) |  None |   1   |   1.3365913848877558  | 0.008929969787656753 |\n",
      "|   awesome   |  None |   1   |   1.133534666034145   | 0.08399643983187526  |\n",
      "|   amazing   |  None |   1   |   1.1000933113660283  | 0.09954776260465983  |\n",
      "|  fantastic  |  None |   1   |   0.8858047568814295  | 0.11167591293399656  |\n",
      "|    great    |  None |   1   |   0.8630655001196618  | 0.018955052444377323 |\n",
      "|     wow     |  None |   1   | -0.009538236067678897 |  0.1604641122471166  |\n",
      "|     bad     |  None |   1   |  -0.9914778800650565  | 0.03848428664699063  |\n",
      "|     hate    |  None |   1   |  -1.3484407222463124  | 0.07715698604297333  |\n",
      "|    awful    |  None |   1   |  -2.0529082040313513  | 0.10099735435259259  |\n",
      "|   terrible  |  None |   1   |   -2.223661436085127  | 0.07731736203785755  |\n",
      "|   horrible  |  None |   1   |   -2.251335236759093  | 0.08020249388788442  |\n",
      "+-------------+-------+-------+-----------------------+----------------------+\n",
      "[12 rows x 5 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_words_model.coefficients.sort('value', ascending=False) \\\n",
    "    .print_rows(num_rows=selected_words_model.coefficients.shape[0],\n",
    "                num_columns=selected_words_model.coefficients.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predicted_sentiment'] = selected_words_model.predict(data,\n",
    "                                                           output_type='probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the accuracy of different sentiment analysis model:** Using the method `.evaluate(test_data)` What is the accuracy of the selected_words_model on the test_data? What was the accuracy of the sentiment_model that we learned using all the word counts in the Jupyter Notebook above from the lectures? What is the accuracy majority class classifier on this task? How do you compare the different learned models with the baseline approach where we are just predicting the majority class? Save these results to answer the quiz at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8463848186404036,\n",
       " 'auc': 0.6936022046674926,\n",
       " 'confusion_matrix': Columns:\n",
       " \ttarget_label\tint\n",
       " \tpredicted_label\tint\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |      1       |        0        |  159  |\n",
       " |      0       |        0        |  371  |\n",
       " |      0       |        1        |  4957 |\n",
       " |      1       |        1        | 27817 |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns],\n",
       " 'f1_score': 0.9157860082304526,\n",
       " 'log_loss': 0.3962265467087378,\n",
       " 'precision': 0.8487520595594068,\n",
       " 'recall': 0.9943165570488991,\n",
       " 'roc_curve': Columns:\n",
       " \tthreshold\tfloat\n",
       " \tfpr\tfloat\n",
       " \ttpr\tfloat\n",
       " \tp\tint\n",
       " \tn\tint\n",
       " \n",
       " Rows: 100001\n",
       " \n",
       " Data:\n",
       " +-----------+--------------------+-----+-------+------+\n",
       " | threshold |        fpr         | tpr |   p   |  n   |\n",
       " +-----------+--------------------+-----+-------+------+\n",
       " |    0.0    |        1.0         | 1.0 | 27976 | 5328 |\n",
       " |   1e-05   |        1.0         | 1.0 | 27976 | 5328 |\n",
       " |   2e-05   |        1.0         | 1.0 | 27976 | 5328 |\n",
       " |   3e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   4e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   5e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   6e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   7e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   8e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " |   9e-05   | 0.9998123123123123 | 1.0 | 27976 | 5328 |\n",
       " +-----------+--------------------+-----+-------+------+\n",
       " [100001 rows x 5 columns]\n",
       " Note: Only the head of the SFrame is printed.\n",
       " You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_words_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority classifier\n",
    "neg_num = test_data[test_data['rating'] == 1].num_rows() \\\n",
    "              + test_data[test_data['rating'] == 2].num_rows()\n",
    "zero_num = test_data[test_data['rating'] == 3].num_rows()\n",
    "pos_num = test_data[test_data['rating'] == 4].num_rows() \\\n",
    "              + test_data[test_data['rating'] == 5].num_rows()\n",
    "total = test_data.num_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative reviews   = 5328\n",
      "Number of undefined opinions = 0\n",
      "Number of positive reviews   = 27976\n",
      "Total of reviews             = 33304\n",
      "Accuracy of the majority     = 0.8400192169108815\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of negative reviews   = {neg_num}')\n",
    "print(f'Number of undefined opinions = {zero_num}')\n",
    "print(f'Number of positive reviews   = {pos_num}')\n",
    "print(f'Total of reviews             = {total}')\n",
    "print(f'Accuracy of the majority     = {pos_num / total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the difference in performance between the models:** To understand why the model with all word counts performs better than the one with only the selected_words, we will now examine the reviews for a particular product.\n",
    "+ We will investigate a product named ‘Baby Trend Diaper Champ’. (This is a trash can for soiled baby diapers, which keeps the smell contained.)\n",
    "+ Just like we did for the reviews for the giraffe toy in the Jupyter Notebook in the lecture video, before we start our analysis you should select all reviews where the product name is ‘Baby Trend Diaper Champ’. Let’s call this table diaper_champ_reviews.\n",
    "+ Again, just as in the video, use the sentiment_model to predict the sentiment of each review in diaper_champ_reviews and sort the results according to their ‘predicted_sentiment’.\n",
    "+ What is the ‘predicted_sentiment’ for the most positive review for ‘Baby Trend Diaper Champ’ according to the sentiment_model from the Jupyter Notebook from lecture? Save this result to answer the quiz at the end.\n",
    "+ Now use the selected_words_model you learned using just the selected_words to predict the sentiment most positive review you found above. Save this result to answer the quiz at the end.\n",
    "+ Why is the predicted_sentiment for the most positive review found using the model with all word counts (sentiment_model) much more positive than the one using only the selected_words (selected_words_model)? Hint: examine the text of this review, the extracted word counts for all words, and the word counts for each of the selected_words, and you will see what each model used to make its prediction.Save this result to answer the quiz at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaper_champ_reviews = data[data['name'] == 'Baby Trend Diaper Champ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaper_champ_reviews = diaper_champ_reviews.sort('predicted_sentiment',\n",
    "                                                 ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981253623335122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaper_champ_reviews['predicted_sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I LOVE LOVE LOVE this product! It is SO much easier to use than the Diaper Genie, (you need a PHD in poopy to figure out how to use the darn thing!) and it even takes the same bags as my kitchen trash can, shich is super convenient, and cost efficient as I can buy them in bulk.The only reason for not rating it a 5 star was that I did have one small problem with it. The foam gasket in the barrell which keeps the poopy smell inside the unit ripped somehow, and it got VERY stinky. HOWEVER, I contacted the manufacturer though their website, and received an email back the same day stating that this was unusual, and that replacement gaskets were on their way to me. They arrived inside of a week and after replacing, it works great again! (They even sent me extras should it happen again)I HIGHLY reccomend this diaper pail over ANY competitors, you will not be sorry!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaper_champ_reviews['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok - newsflash.  Diapers are just smelly.  We've had this pail for 2.5 years now.  It was our first and primary one.  There were no major smell problems until after one year, when our son started eating solids.  Also, we change the bag twice weekly as 3 days is about the max for smell-containment.  Around 20-22 months we started shopping for a container that would be less smelly and didn't find one as good.  (We have a cheaper one upstairs which broke immediately and always stunk!)  We finally just put the Diaper Champ in the attic a few months ago and use the cheap one with the flip-up lid - mainly since the cheapo fits inside the cabinet and we didn't notice a big difference in smell-control.  (The most helpful action is to tie the dirty diapers inside a small plastic bag before putting them in the pail.)A couple of our friends have this pail and were pleased until the children started eating solid food and things got stinkier - but that's pretty much the consensus according to many reviews on the different pails.  I've knocked off a star since the mechanism isn't that intuitive (to grandmothers and babysitters, at least) and it frequently gets stuck when the bag starts getting full.  Plus there are some smell issues, as with any pail.  However, the fact that you can use regular trash bags is so much more convenient!\n",
      "idx = 240\n"
     ]
    }
   ],
   "source": [
    "# Most positive review in sentiment_model\n",
    "for idx, rev in enumerate(diaper_champ_reviews):\n",
    "    if diaper_champ_reviews['review'][idx][:3] == 'Ok ':\n",
    "        print(diaper_champ_reviews[idx]['review'])\n",
    "        print(f'idx = {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7919288370624453"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaper_champ_reviews['predicted_sentiment'][240]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
